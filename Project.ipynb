{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LSTM, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to parse data from a file\n",
    "def parse_data(file_path):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            cleaned_line = line.strip()\n",
    "            if cleaned_line in {\"end\", \"<end>\"}:\n",
    "                sequences.append(\"end\")\n",
    "                labels.append(\"end\")\n",
    "            elif len(cleaned_line.split()) == 2:\n",
    "                parts = cleaned_line.split()\n",
    "                sequences.append(parts[0])  # Amino acid\n",
    "                labels.append(parts[1])  # Secondary structure label\n",
    "            else:\n",
    "                continue  # Skip any malformed lines\n",
    "    return sequences, labels\n",
    "\n",
    "# Function to encode sequences\n",
    "def encode_sequences(sequences):\n",
    "    amino_acid_mapping = {'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'Q': 5, 'E': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19, 'end': 20}\n",
    "    encoded_seqs = np.zeros((len(sequences), 21))\n",
    "    for idx, amino_acid in enumerate(sequences):\n",
    "        encoded_seqs[idx, amino_acid_mapping[amino_acid]] = 1\n",
    "    return encoded_seqs\n",
    "\n",
    "# Function to encode labels\n",
    "def encode_labels(labels):\n",
    "    label_mapping = {'e': 0, 'h': 1, '_': 2, 'end': 3}\n",
    "    encoded_labels = np.zeros((len(labels), 3))  # No need for encoding 'end'\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label != 'end':\n",
    "            encoded_labels[idx, label_mapping[label]] = 1\n",
    "    return encoded_labels\n",
    "\n",
    "# Load and process training and test data\n",
    "seq_train, labels_train = parse_data(\"protein-secondary-structure.train.txt\")\n",
    "seq_test, labels_test = parse_data(\"protein-secondary-structure.test.txt\")\n",
    "\n",
    "\n",
    "eseq_train = encode_sequences(seq_train)\n",
    "elab_train = encode_labels(labels_train)\n",
    "eseq_test = encode_sequences(seq_test)\n",
    "elab_test = encode_labels(labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to create sliding windows\n",
    "def create_sliding_windows(encoded_seqs, window_size=13):\n",
    "    padding = np.zeros((window_size // 2, 21))\n",
    "    padded_seqs = np.vstack([padding, encoded_seqs, padding])\n",
    "    windows = np.array([padded_seqs[i:i + window_size].flatten() for i in range(len(encoded_seqs))])\n",
    "    return windows\n",
    "\n",
    "# Prepare windowed input for model\n",
    "windowed_input_train = create_sliding_windows(np.array(eseq_train))\n",
    "windowed_input_test = create_sliding_windows(np.array(eseq_test))\n",
    "\n",
    "def build_first_network(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(40, activation='sigmoid', input_shape=(input_shape,)),  # Hidden layer with 40 units\n",
    "        Dense(3, activation='softmax')  # Output layer for the three types of secondary structures\n",
    "    ])\n",
    "    model.compile(optimizer=SGD(),  # Stochastic gradient descent optimizer\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "def build_second_network():\n",
    "    model = Sequential([\n",
    "        Dense(40, activation='sigmoid', input_shape=(3,)),  # Input shape matches the output of the first network\n",
    "        Dense(3, activation='softmax')  # Same output configuration as the first network\n",
    "    ])\n",
    "    model.compile(optimizer=SGD(),  # Using the same optimizer as the first network\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "def train_cascade(first_model, second_model, X_train, y_train, X_test, y_test):\n",
    "    # Train the first model and save the history\n",
    "    history1 = first_model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Generate intermediate outputs from the first model\n",
    "    intermediate_output = first_model.predict(X_train)\n",
    "    \n",
    "    # Train the second model using the outputs of the first model as inputs, and save the history\n",
    "    history2 = second_model.fit(intermediate_output, y_train, epochs=50, validation_data=(first_model.predict(X_test), y_test))\n",
    "    \n",
    "    return history1, history2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = windowed_input_train\n",
    "y_train = elab_train\n",
    "X_test = windowed_input_test\n",
    "y_test = elab_test\n",
    "\n",
    "# Define and train models\n",
    "first_network = build_first_network(input_shape=X_train.shape[1])\n",
    "second_network = build_second_network()\n",
    "\n",
    "# Train both models and get their histories\n",
    "history1, history2 = train_cascade(first_network, second_network, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Evaluate models\n",
    "intermediate_output_test = first_network.predict(X_test)\n",
    "test_loss, test_accuracy = second_network.evaluate(intermediate_output_test, y_test)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(7, 12))  # Adjusted figure size for vertical layout\n",
    "\n",
    "# Subplot for the accuracy of second model\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(history2.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history2.history['val_accuracy'], label='Test Accuracy')\n",
    "plt.title('Cascaded Network Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Subplot for the loss of second model\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(history2.history['loss'], label='Train Loss')\n",
    "plt.plot(history2.history['val_loss'], label='Test Loss')\n",
    "plt.title('Cascaded Network Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = second_network.predict(intermediate_output_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)  # Use the encoded labels for y_test\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "labels = ['e', 'h', '_']\n",
    "\n",
    "# Plotting the normalized confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap='Reds', cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to create sliding windows\n",
    "def create_sliding_windows(encoded_seqs, window_size=13):\n",
    "    padding = np.zeros((window_size // 2, 21))  # Zero vectors for padding\n",
    "    padded_seqs = np.vstack([padding, encoded_seqs, padding])\n",
    "    windows = np.array([padded_seqs[i:i + window_size] for i in range(len(encoded_seqs))])\n",
    "    return windows\n",
    "\n",
    "# Assuming the presence of functions parse_data, encode_sequences, and encode_labels\n",
    "# Load and process training data\n",
    "seq_train, labels_train = parse_data(\"protein-secondary-structure.train.txt\")\n",
    "eseq_train = encode_sequences(seq_train)\n",
    "elab_train = encode_labels(labels_train)\n",
    "windowed_input_train = create_sliding_windows(np.array(eseq_train), window_size=13)\n",
    "\n",
    "# Load and process testing data\n",
    "seq_test, labels_test = parse_data(\"protein-secondary-structure.test.txt\")\n",
    "eseq_test = encode_sequences(seq_test)\n",
    "elab_test = encode_labels(labels_test)\n",
    "windowed_input_test = create_sliding_windows(np.array(eseq_test), window_size=13)\n",
    "\n",
    "# Function to build the second model (cascade)\n",
    "def build_second_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(40, activation='sigmoid', input_shape=(input_shape,)),  # Input shape matches the output of the first network\n",
    "        Dense(3, activation='softmax')  # Same output configuration as the first network\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(),  # Using Adam optimizer\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Assuming the presence of the train_cascade function\n",
    "\n",
    "# Build the first model (Convolutional Neural Network)\n",
    "def build_improved_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='sigmoid', input_shape=(input_shape[0], input_shape[1]),\n",
    "               kernel_regularizer=l2(0.01)),  # Single Conv1D layer with increased filters and L2 regularization\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(40, activation='sigmoid', kernel_regularizer=l2(0.01)),  # L2 regularization on Dense layer\n",
    "        Dropout(0.3),  # Dropout to prevent overfitting\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    optim = Adam(learning_rate=0.0001)  # Optimizer with a conservative learning rate\n",
    "    model.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build the cascade architecture\n",
    "def build_cascade_model(first_model, second_model):\n",
    "    # Train the first model\n",
    "    history1 = first_model.fit(windowed_input_train, elab_train, epochs=25, validation_data=(windowed_input_test, elab_test), batch_size=32)\n",
    "    \n",
    "    # Generate intermediate outputs from the first model\n",
    "    intermediate_output = first_model.predict(windowed_input_train)\n",
    "    \n",
    "    # Train the second model using the outputs of the first model\n",
    "    history2 = second_model.fit(intermediate_output, elab_train, epochs=25, validation_data=(first_model.predict(windowed_input_test), elab_test), batch_size=32)\n",
    "    \n",
    "    return history1, history2\n",
    "\n",
    "# Build and train both models\n",
    "better_first_model = build_improved_model(windowed_input_train.shape[1:])\n",
    "better_second_model = build_second_model(better_first_model.output_shape[1])\n",
    "better_history1, better_history2 = build_cascade_model(better_first_model, better_second_model)\n",
    "\n",
    "# Evaluate the cascade architecture\n",
    "intermediate_output_test = better_first_model.predict(windowed_input_test)\n",
    "test_loss, test_accuracy = better_second_model.evaluate(intermediate_output_test, elab_test)\n",
    "print(\"Test accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a figure to host two subplots vertically\n",
    "plt.figure(figsize=(7, 12))  # Adjusted figure size for vertical layout\n",
    "\n",
    "# Subplot for the accuracy\n",
    "plt.subplot(2, 1, 1)  # 2 rows, 1 column, 1st subplot (top)\n",
    "plt.plot(history2.history['accuracy'], label='Train Accuracy')  # Use history2 for the second model\n",
    "plt.plot(history2.history['val_accuracy'], label='Validation Accuracy')  # Use history2 for the second model\n",
    "plt.title('Improved Cascaded Network Accuracy')  # Update the title\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Subplot for the loss\n",
    "plt.subplot(2, 1, 2)  # 2 rows, 1 column, 2nd subplot (bottom)\n",
    "plt.plot(history2.history['loss'], label='Train Loss')  # Use history2 for the second model\n",
    "plt.plot(history2.history['val_loss'], label='Validation Loss')  # Use history2 for the second model\n",
    "plt.title('Improved Cascaded Network Loss')  # Update the title\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the second network (Improved Cascaded Network)\n",
    "y_pred = second_network.predict(intermediate_output_test)  # Use intermediate_output_test from the cascade architecture\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(elab_test, axis=1)  # Use the encoded labels for elab_test\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "labels = ['e', 'h', '_']\n",
    "\n",
    "# Plotting the normalized confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap='Reds', cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Normalized Confusion Matrix for Improved Cascaded Network')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test on unseen data sheets\n",
    "#RS126Data, C : loops\n",
    "#H : Helix\n",
    "#E : Stand\n",
    "\n",
    "# Load and process testing data\n",
    "def parse_unseen_data(file_path):\n",
    "    '''\n",
    "    There is 150 instance and every instance contain 2 line .\n",
    "    1st line = primary sequence (amino acid)\n",
    "    2nd line = secondary sequence (C,H,E)\n",
    "    #FNKEQQNAFYEILHLPNLNEEQRNGFIQSLKDDPSQSANLLAE\n",
    "    CCCCHHHHHHHHHCCCCCCCHHHHHHHHHHHHCCCCCCCCCCC'''\n",
    "    sequences = []\n",
    "    labels = []\n",
    " \n",
    "    with open(file_path, \"r\") as file:\n",
    "        # Read the file line by line\n",
    "        lines = file.readlines()\n",
    "        #print(len(lines)    )\n",
    "        # Iterate over the lines, 2 at a time\n",
    "        primary = True\n",
    "        for line in lines:\n",
    "            cleaned_line = line.strip()\n",
    "            if primary:\n",
    "                for charac in cleaned_line:\n",
    "                    sequences.append(charac)\n",
    "                #sequences.append(\"end\")\n",
    "                primary = False\n",
    "            else:\n",
    "                for charac in cleaned_line:\n",
    "                    labels.append(charac)\n",
    "                #labels.append(\"end\")\n",
    "                primary = True\n",
    "    \n",
    "\n",
    "    return sequences, labels\n",
    "\n",
    "def convert_unseen_labels(labels):\n",
    "    '''\n",
    "    Convert unseen labels to numerical format.\n",
    "    C : _\n",
    "    H : h\n",
    "    E : e\n",
    "    '''\n",
    "    label_mapping = {'C': '_', 'H': 'h', 'E': 'e', 'end': 'end'}\n",
    "    converted_labels = [label_mapping[label] for label in labels]\n",
    "    return converted_labels\n",
    "\n",
    "def create_sliding_windows(encoded_seqs, window_size=13):\n",
    "    padding = np.zeros((window_size // 2, 21))\n",
    "    padded_seqs = np.vstack([padding, encoded_seqs, padding])\n",
    "    # Flatten each window slice to make it compatible with the model input\n",
    "    windows = np.array([padded_seqs[i:i + window_size].flatten() for i in range(len(encoded_seqs))])\n",
    "    return windows\n",
    "\n",
    "#check_label_amino_length(\"RS126.data.txt\")\n",
    "seq, labels = parse_unseen_data(\"RS126.data.txt\")\n",
    "labels = convert_unseen_labels(labels)\n",
    "# Encoding sequences and labels\n",
    "eseq = encode_sequences(seq)\n",
    "elab = encode_labels(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "eseq_train, eseq_test, elab_train, elab_test = train_test_split(eseq, elab, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create sliding windows\n",
    "window_size = 13\n",
    "windowed_input_train = create_sliding_windows(eseq_train, window_size)\n",
    "windowed_input_test = create_sliding_windows(eseq_test, window_size)\n",
    "\n",
    "# Build and train models\n",
    "input_shape = window_size * 21  # Adjust according to your encoding size\n",
    "\n",
    "X_train = windowed_input_train\n",
    "y_train = elab_train\n",
    "X_test = windowed_input_test\n",
    "y_test = elab_test\n",
    "# Test the two previous models made on the unseen data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq, labels = parse_unseen_data(\"RS126.data.txt\")\n",
    "labels = convert_unseen_labels(labels)\n",
    "eseq = encode_sequences(seq)\n",
    "elab = encode_labels(labels)\n",
    "\n",
    "# Create sliding windows\n",
    "window_size = 13\n",
    "windowed_input_test = create_sliding_windows(eseq, window_size)\n",
    "\n",
    "# Assuming `first_network` and `second_network` are your original cascade models\n",
    "# Assuming `first_model` and `second_model` are your improved cascade models\n",
    "\n",
    "# Test the original cascade model\n",
    "intermediate_output_test_orig = first_network.predict(windowed_input_test)\n",
    "test_loss_orig, test_accuracy_orig = second_network.evaluate(intermediate_output_test_orig, elab)\n",
    "print(\"Original Cascade Model Test accuracy on unseen data:\", test_accuracy_orig)\n",
    "\n",
    "# Test the improved cascade model\n",
    "intermediate_output_test_improved = better_first_model.predict(windowed_input_test)\n",
    "test_loss_improved, test_accuracy_improved = better_second_model.evaluate(intermediate_output_test_improved, elab)\n",
    "print(\"Improved Cascade Model Test accuracy on unseen data:\", test_accuracy_improved)\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    labels = ['e', 'h', '_']\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap='Reds', cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "# Predict and plot confusion matrix for original cascade model\n",
    "y_pred_orig = second_network.predict(intermediate_output_test_orig)\n",
    "y_pred_classes_orig = np.argmax(y_pred_orig, axis=1)\n",
    "y_true_classes = np.argmax(elab, axis=1)\n",
    "plot_confusion_matrix(y_true_classes, y_pred_classes_orig, 'Original Cascade Model Normalized Confusion Matrix')\n",
    "\n",
    "# Predict and plot confusion matrix for improved cascade model\n",
    "y_pred_improved = better_second_model.predict(intermediate_output_test_improved)\n",
    "y_pred_classes_improved = np.argmax(y_pred_improved, axis=1)\n",
    "plot_confusion_matrix(y_true_classes, y_pred_classes_improved, 'Improved Cascade Model Normalized Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Original Cascade Model', 'Improved Cascade Model']\n",
    "accuracies = [test_accuracy_orig, test_accuracy_improved]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, accuracies, color=['blue', 'green'])\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy of Original vs Improved Cascade Models')\n",
    "for i, v in enumerate(accuracies):\n",
    "    plt.text(i, v + 0.01, f\"{v:.2%}\", ha='center', va='bottom', fontsize=12)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
